{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Headline_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TnrY31RtjXs7"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ujp6-vIxjXsC",
        "colab": {}
      },
      "source": [
        "!pip install rouge==0.3.1\n",
        "!pip install tokenizers\n",
        "!pip install pytorch-lightning\n",
        "!pip install -U transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EIEA8wbxjXsV",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PD1iPbpmjXsc",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYyc8kpOAucU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eLZ8dJ-BjXsg"
      },
      "source": [
        "### Three news datasets: 'Газета', 'РИА Новости', 'Свободная пресса'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f97I0kDKwRLX"
      },
      "source": [
        "\"РИА Новости\", \"Свободная пресса\" gives only text and headlines, \"Газета\" consist of text, summary, headlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ehKA41mzjXsh",
        "colab": {}
      },
      "source": [
        "!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n",
        "!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n",
        "!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7Bf5y_vXmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uqwz3T2mjXsl",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Huawei')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N48xCasvjXso",
        "colab": {}
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_SnfkODfkvWr",
        "colab": {}
      },
      "source": [
        "def read_gazeta_records(file_name, shuffle = True):\n",
        "    records = []\n",
        "    with open(file_name, \"r\") as r:\n",
        "        for line in r:\n",
        "            records.append(eval(line))\n",
        "    records = pd.DataFrame(records)\n",
        "    if shuffle:\n",
        "        records = records.sample(frac=1)\n",
        "    return records"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rTlWiPLQjXsr",
        "colab": {}
      },
      "source": [
        "data_rio = pd.read_csv('data_rio.csv')\n",
        "data_svbpress = pd.read_csv('data_main.csv')\n",
        "data_rio = data_rio.drop_duplicates()\n",
        "data_svbpress.drop(\"Unnamed: 0\", axis='columns', inplace=True)\n",
        "data_svbpress.columns = ['text', 'title']\n",
        "data_svbpress['text'] = data_svbpress['text'].apply(lambda x: str(x))\n",
        "data_rio['text'] = data_rio['text'].apply(lambda x: str(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y85yvpS8mUy2",
        "colab": {}
      },
      "source": [
        "train_gazeta = read_gazeta_records(\"gazeta_train.txt\")\n",
        "val_gazeta = read_gazeta_records(\"gazeta_val.txt\")\n",
        "test_gazeta = read_gazeta_records(\"gazeta_test.txt\")\n",
        "train_gazeta.drop(['url','date'], axis='columns', inplace=True)\n",
        "val_gazeta.drop(['url','date'], axis='columns', inplace=True)\n",
        "test_gazeta.drop(['url','date'], axis='columns', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rZMZIULmjXsu",
        "colab": {}
      },
      "source": [
        "data_svbpress.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8dZZ4OCujXsx",
        "colab": {}
      },
      "source": [
        "data_rio.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D9-p1WwhtIt6",
        "colab": {}
      },
      "source": [
        "train_gazeta.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0GzGzYQUjXsz",
        "colab": {}
      },
      "source": [
        "X_train_svbpress, X_val_svbpress = train_test_split(data_svbpress.iloc[2000:], test_size=0.2)\n",
        "X_test_svbpress = data_svbpress.iloc[:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qObi1mC_jXs2",
        "colab": {}
      },
      "source": [
        "X_train_rio, X_val_rio = train_test_split(data_rio.iloc[5000:], test_size=0.2)\n",
        "X_test_rio = data_rio.iloc[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGV-Yb2ibosM",
        "colab_type": "text"
      },
      "source": [
        "Statistics for datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSjhPHbibr7K",
        "colab_type": "code",
        "outputId": "75c629d9-36e4-4f00-91e7-e7b831817686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(f'gazeta: train:{len(train_gazeta)}| val:{len(val_gazeta)}| test:{len(test_gazeta)}')\n",
        "print(f'rio: train:{len(X_train_rio)}| val:{len(X_val_rio)}| test:{len(X_test_rio)}')\n",
        "print(f'svbpress: train:{len(X_train_svbpress)}| val:{len(X_val_svbpress)}| test:{len(X_test_svbpress)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gazeta: train:52400| val:5265| test:5770\n",
            "rio: train:73963| val:18491| test:5000\n",
            "svbpress: train:22312| val:5579| test:2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1bvPmzBfxddH"
      },
      "source": [
        "I will use 2 metrics: BLEU and ROUGE. They more correlate with human evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Cl29_xkjXs5",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "def calc_scores(references, predictions):\n",
        "    Metrics = namedtuple(\"Metrics\", \"BLEU, ROUGE\")\n",
        "    print(\"Ref:\", references[-1])\n",
        "    print(\"Hyp:\", predictions[-1])\n",
        "    \n",
        "    Metrics.BLEU = corpus_bleu([[r] for r in references], predictions)\n",
        "    print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(predictions, references, avg=True)\n",
        "    Metrics.ROUGE = scores\n",
        "    print(\"ROUGE: \", scores)\n",
        "    return Metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TnrY31RtjXs7"
      },
      "source": [
        "### Baseline lead rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gTE0gBNxjXs9",
        "colab": {}
      },
      "source": [
        "def calc_lead_rows_score(data, n=1, lower=True, summary = 'title'):\n",
        "    references = []\n",
        "    predictions = []\n",
        "\n",
        "    for text, summary in data[['text', summary]].values:\n",
        "        summary = summary if not lower else summary.lower()\n",
        "        references.append(summary)\n",
        "\n",
        "        text = text if not lower else text.lower()\n",
        "        sentences = [sentence for sentence in sent_tokenize(text)] \n",
        "        prediction = \" \".join(sentences[:n])\n",
        "        predictions.append(prediction)\n",
        "    \n",
        "    return calc_scores(references, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPP2Rfc_4Alm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_svbpress"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XmqTod3vjXs_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7cafa4f1-e686-47d8-a1b1-39311242233f"
      },
      "source": [
        "scores_svbpress = calc_lead_rows_score(X_test_svbpress, n=1)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ref: фсб предотвратила теракты на юге россии\n",
            "Hyp: сотрудники фсб во взаимодействии с коллегами из мвд пресекли деятельность действовавшей на территории махачкалы и грозного законспирированной ячейки сторонников террористической организации «исламское государство» *.\n",
            "BLEU:  0.1562924441829961\n",
            "ROUGE:  {'rouge-1': {'f': 0.20511201241971977, 'p': 0.15161266466652415, 'r': 0.4164164821289826}, 'rouge-2': {'f': 0.0760298004291592, 'p': 0.05572862101967875, 'r': 0.16833497474747466}, 'rouge-l': {'f': 0.14880496841140162, 'p': 0.1393586631944527, 'r': 0.3852106282606283}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xz1EQazOjXtB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "62d1c45a-93c9-42e8-afb4-2f5c993b4d82"
      },
      "source": [
        "scores_rio = calc_lead_rows_score(X_test_rio, n=1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ref: техрегулирование - тема заседания комиссии по модернизации экономики\n",
            "Hyp:  президент рф дмитрий медведев проведет в липецке первое в этом году заседание комиссии по модернизации экономики, посвященное совершенствованию системы технического регулирования, сообщает пресс-служба кремля.\n",
            "BLEU:  0.20250873974111167\n",
            "ROUGE:  {'rouge-1': {'f': 0.23528978247089313, 'p': 0.1629620222229872, 'r': 0.4628657885088771}, 'rouge-2': {'f': 0.10134627488506141, 'p': 0.06891367686074115, 'r': 0.21393361027860888}, 'rouge-l': {'f': 0.16181793878661085, 'p': 0.15046233323360608, 'r': 0.4261026888307769}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcVdnC4wthkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "afe09173-a218-4199-a2b7-e079552851b7"
      },
      "source": [
        "scores_gazeta = calc_lead_rows_score(test_gazeta, n=3, summary = 'summary')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ref: в ходе спецоперации на территории москвы и республики дагестан силовики задержали четверых участников банды «шараповские» — были изъяты оружие, гранаты и противотанковые комплексы. по данным ск, члены опг причастны к совершению серии убийств предпринимателей и сотрудников правоохранительных органов, а также к похищениям людей, вымогательствам, грабежам и разбоям. кроме того, предполагаемым злоумышленникам вменяют организацию в 1998 году теракта в дагестане.\n",
            "Hyp: в москве и дагестане следователи ск совместно с сотрудниками фсб и мвд задержали четверых участников организованной преступной группы «шараповские». об этом тасс сообщила старшая помощница руководителя главного следственного управления ск по москве юлия иванова. «с августа по ноябрь на территории республики дагестан и москвы задержаны четыре соучастника преступной группировки», — отметила иванова.\n",
            "BLEU:  0.43414697189598955\n",
            "ROUGE:  {'rouge-1': {'f': 0.2631246381422198, 'p': 0.2513271664548712, 'r': 0.29428704269831274}, 'rouge-2': {'f': 0.112157867529966, 'p': 0.10605332489420513, 'r': 0.12840323465681316}, 'rouge-l': {'f': 0.22643633572647107, 'p': 0.2286673284660168, 'r': 0.2673145144842171}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ahgupuFw09Q"
      },
      "source": [
        "## BART model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZa7vhJakXwL"
      },
      "source": [
        "**Pipeline**:\n",
        "1. Download and work out with datasets.\n",
        "2. Train BPE tokenizer 50000 rows will be enough for training.\n",
        "3. Split dataset and corrupt it.\n",
        "4. Choose config that is suitable for these datasets.\n",
        "5. Save model's weights .\n",
        "6. Train, then generate using greedy/beam search.\n",
        "7. Repeat experiement few times for more reliable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5YmbV9KuxWEq",
        "colab": {}
      },
      "source": [
        "import transformers\n",
        "from transformers import BartTokenizer\n",
        "from transformers import BartModel, BartConfig, GPT2Tokenizer, BartForConditionalGeneration, AutoTokenizer, BartTokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.processors import BertProcessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3xI2O1mp86s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train([\"/content/drive/My Drive/Huawei/data_bpe.txt\"], vocab_size=30000, special_tokens=[\"<unk>\",\"<pad>\", \"<mask>\", \"<s>\", \"</s>\"])\n",
        "tokenizer.save('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb76ugDuhmF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BartTokenizer.from_pretrained('/content/drive/My Drive/Huawei/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiJwUO7UkETg",
        "colab_type": "text"
      },
      "source": [
        "Number of tokens for each dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGXQjsbkJah",
        "colab_type": "code",
        "outputId": "a39b213e-cb42-4b8e-90a3-8f10ae9cbcba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "for data in [X_test_rio, X_train_rio, X_val_rio, X_val_svbpress, X_train_svbpress, X_test_svbpress, train_gazeta, val_gazeta, test_gazeta]:\n",
        "  stat = tokenizer.encode_batch(data['text'].values.tolist())\n",
        "  len_ = 0\n",
        "  for i in stat:\n",
        "    len_ += len(i.ids)\n",
        "  print(f'num_tokens:{len_}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_tokens:1847332\n",
            "num_tokens:25769188\n",
            "num_tokens:6438205\n",
            "num_tokens:1097992\n",
            "num_tokens:4400700\n",
            "num_tokens:411171\n",
            "num_tokens:61405676\n",
            "num_tokens:6157595\n",
            "num_tokens:6548405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP62sxhapqbE",
        "colab_type": "text"
      },
      "source": [
        "Out of Vocab rate for each dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUU8EbJIpp8v",
        "colab_type": "code",
        "outputId": "8ef1c77e-4030-49f3-d9e8-5bde92429200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "for data in [X_train_rio, X_val_rio, X_test_rio, X_train_svbpress, X_val_svbpress, X_test_svbpress, train_gazeta, val_gazeta, test_gazeta]:\n",
        "  stat = tokenizer.encode_batch(data['text'].values.tolist())\n",
        "  unknown = 0\n",
        "  len_ = 0\n",
        "  for i in stat:\n",
        "    unknown += np.sum(np.array(i.ids) == tokenizer.token_to_id('<unk>'))\n",
        "    len_ += len(i.ids)\n",
        "  print(f'out_of_vocab_rate:{unknown/len_*100}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n",
            "out_of_vocab_rate:0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-soA4H_9Ywg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class PretrainingDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               data,\n",
        "               max_seq_length = 300,\n",
        "               sentence_permutation = True,\n",
        "               token_masking = False,\n",
        "               token_deletion = False, \n",
        "               text_infilling = True):\n",
        "    \n",
        "    self.data = data\n",
        "    self.pad = 1\n",
        "    self.mask = 2\n",
        "    self.sos = 3\n",
        "    self.eos = 4\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.sentence_permutation = sentence_permutation\n",
        "    self.token_deletion = token_deletion\n",
        "    self.token_masking = token_masking\n",
        "    self.text_infilling = text_infilling\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    x = self.data[idx]\n",
        "    target = tokenizer.encode_plus(x)['input_ids']\n",
        "\n",
        "    if self.sentence_permutation:\n",
        "        split_regex = re.compile(r'[.|!|?|…]')\n",
        "        sentenced = list(filter(lambda t: t, [t.strip() for t in split_regex.split(x)]))\n",
        "        x = random.sample(sentenced, len(sentenced))\n",
        "        x = ' '.join(x)\n",
        "    x = tokenizer.encode_plus(x)['input_ids'][1:-1]\n",
        "\n",
        "    if self.token_deletion:\n",
        "        len_ = len(x)\n",
        "        idx = random.sample(np.arange(len_).tolist(), int(len_*0.15))\n",
        "        for i in idx:\n",
        "          x.remove(x[i])\n",
        "\n",
        "    if self.token_masking:\n",
        "        x = np.array(x)\n",
        "        len_ = len(x)\n",
        "        idx = random.sample(np.arange(len_).tolist(), int(len_*0.15))\n",
        "        x[idx] = self.mask\n",
        "        x = x.tolist()\n",
        "\n",
        "    if self.text_infilling: \n",
        "        x = np.array(x)\n",
        "        len_ = len(x)\n",
        "        number_of_spans = random.choice(range(5,8))\n",
        "        idx = random.sample(np.arange(len_).tolist(), number_of_spans)\n",
        "        poissons = np.random.poisson(3, number_of_spans)\n",
        "        new_idx = zip(idx, poissons)\n",
        "        copy_idx = deepcopy(new_idx)\n",
        "        x[np.concatenate([np.arange(i, min(i+j+1, len(x))) for i, j in new_idx])] = -1\n",
        "        x = x.tolist()\n",
        "        for i, _ in list(copy_idx):\n",
        "          x.insert(i, self.mask)\n",
        "        x = np.array(x)\n",
        "        x = x[x!=-1].tolist()\n",
        "    \n",
        "    x = [self.sos] + x + [self.eos]\n",
        "    attention_mask = [1]*len(x)\n",
        "    if len(x)>= self.max_seq_length:\n",
        "      x = x[:self.max_seq_length-1]\n",
        "      x = x + [self.eos]\n",
        "      attention_mask = attention_mask[:self.max_seq_length]\n",
        "    else:\n",
        "      x += [self.pad]*(self.max_seq_length-len(x))\n",
        "      attention_mask += [0]*(self.max_seq_length-len(attention_mask))\n",
        "    \n",
        "    if len(target)>= self.max_seq_length:\n",
        "      target = target[:self.max_seq_length-1]\n",
        "      target = target + [self.eos]\n",
        "    else:\n",
        "      target += [self.pad]*(self.max_seq_length-len(target))\n",
        "    \n",
        "    x = torch.LongTensor(x)\n",
        "    attention_mask = torch.LongTensor(attention_mask)\n",
        "    target = torch.LongTensor(target)\n",
        "    lm_labels = target[:, 1:].clone()\n",
        "    lm_labels[target[:, 1:] == 1] = -100\n",
        "\n",
        "    return {\n",
        "            'input_ids': x,\n",
        "            'attention_mask': attention_mask,\n",
        "            'decoder_input_ids': target,\n",
        "            'lm_labels': lm_labels\n",
        "            }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ0K6tGaqhGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_pret = pd.read_csv('/content/drive/My Drive/Huawei/data_pret_small.csv')['text'].values\n",
        "large_pret = pd.read_csv('/content/drive/My Drive/Huawei/data_pret_large.csv')['text'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_djJ7aUiZd0m",
        "colab_type": "text"
      },
      "source": [
        "Let's count out of vocab n-gram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bCWLWDvZrsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUV_dataset = PretrainingDataset(large_pret)\n",
        "OUV_loader = DataLoader(OUV_dataset, batch_size=128)\n",
        "unknown = 0\n",
        "for batch in OUV_loader:\n",
        "  unknown += torch.sum(batch['input_ids'].flatten() == tokenizer.token_to_id('<unk>'))\n",
        "print(f'Out of vocab:{unknown}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIaZGpZHVN0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val = train_test_split(large_pret)\n",
        "train_dataset = PretrainingDataset(X_train)\n",
        "val_dataset = PretrainingDataset(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZCZvpIOEmVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 7\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxqLSbyRKKZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 7\n",
        "test_dataset = PretrainingDataset(small_pret)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDqtlhEJcdlC",
        "colab_type": "text"
      },
      "source": [
        "Usually for generation use beam-search and loss with regularization for repeated n-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKmMse2yy_0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = BartConfig(vocab_size=tokenizer.vocab_size, \n",
        "                    pad_token_id = 1,\n",
        "                    bos_token_id=3,\n",
        "                    eos_token_id=4,\n",
        "                    encoder_layers = 6,\n",
        "                    decoder_layers = 6\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_jicqJ3B-I",
        "colab_type": "text"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv8bqPB2LF5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class BART(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.bart = BartForConditionalGeneration(config)\n",
        "\n",
        "    def forward(self, decoder_input_ids = None, input_ids=None, attention_mask=None):\n",
        "        bart_output = self.bart(input_ids=input_ids,\n",
        "                                attention_mask=attention_mask,\n",
        "                                decoder_input_ids = decoder_input_ids)\n",
        "        return bart_output[0]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset, batch_size=self.hparams.batch_size, shuffle=True)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.batch_size, shuffle=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma = 0.7)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, attn, y = batch['input_ids'], batch['attention_mask'], batch['decoder_input_ids']\n",
        "        loss = self(decoder_input_ids = y[:, :-1], input_ids = x, attention_mask = attn)\n",
        "\n",
        "        #output_dim = logits.shape[-1]\n",
        "            \n",
        "        #output = logits.reshape(-1, output_dim)\n",
        "        #trg = y[:, 1:].reshape(-1)\n",
        "\n",
        "        #loss = criterion(output, trg)\n",
        "\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        \n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, attn, y = batch['input_ids'], batch['attention_mask'], batch['decoder_input_ids']\n",
        "        loss = self(decoder_input_ids = y[:, :-1], input_ids = x, attention_mask = attn)\n",
        "\n",
        "        #output_dim = logits.shape[-1]\n",
        "        #predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        #ref = tokenizer.batch_decode(y)\n",
        "        #pred = tokenizer.batch_decode(predictions)\n",
        "        #bleu = corpus_bleu([[r] for r in ref], pred) \n",
        "            \n",
        "        #output = logits.reshape(-1, output_dim)\n",
        "        #trg = y[:, 1:].reshape(-1)\n",
        "\n",
        "        #loss = criterion(output, trg)\n",
        "        #acc = accuracy_score(torch.argmax(output, dim=-1).cpu(), trg.cpu())\n",
        "        #acc = torch.tensor(acc)\n",
        "        #bleu = torch.FloatTensor([bleu])\n",
        "\n",
        "        return {'val_loss': loss}\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "        avg_bleu = torch.stack([x['bleu'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc, 'avg_bleu': avg_bleu}\n",
        "        return {'val_loss': avg_loss, 'avg_bleu': avg_bleu, 'progress_bar': tensorboard_logs}\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2aDze52OiK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning import seed_everything\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from argparse import Namespace\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "seed_everything(42)\n",
        "args = {\n",
        "    'batch_size': 16,\n",
        "    'lr': 0.0001 \n",
        "}\n",
        "hparams = Namespace(**args)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='/content/drive/My Drive/Huawei/lightning_logs/pretraining with pad/checkpoints',\n",
        "    save_top_k=True,\n",
        "    verbose=True,\n",
        "    monitor='avg_bleu',\n",
        "    mode='max',\n",
        "    prefix=''\n",
        ")\n",
        "\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='avg_bleu',\n",
        "    min_delta=0.00,\n",
        "    patience=3,\n",
        "    verbose=False,\n",
        "    mode='max'\n",
        ")\n",
        "model = BART(hparams)\n",
        "trainer = Trainer(gpus=1, max_epochs=15, \n",
        "                  early_stop_callback=early_stop_callback, deterministic=True,checkpoint_callback=checkpoint_callback, auto_lr_find=True)\n",
        "trainer.fit(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72jtKlDPkpjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n3BuP0CEJfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DApDCfTQBr1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /lightning_logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OokmHVK3MT3",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlJl5lbSFD1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldG_I4kr3Q3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BART.load_from_checkpoint(checkpoint_path=\"/content/drive/My Drive/Huawei/lightning_logs/pretraining with pad/checkpoints/epoch=10.ckpt\")\n",
        "model_pretrained = model.bart"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPFiGUt_iTiZ",
        "colab_type": "code",
        "outputId": "0d80584a-c9e6-455d-875e-64457c871379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = X_train_rio['title'].apply(lambda x: len(word_tokenize(x))).values\n",
        "bins = len(np.unique(x))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.hist(x, bins)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQkklEQVR4nO3df6zddX3H8edrBZxBDUW6pmm7lWmTBc1WsYEumoVpVgr+UUwMgWTSGWJNLAlm/kH1nzKUpC5TNxJlqaOxJGptBKXRutoQEucfYC9YgcIYd1hCm9JeLYjERIO+98f53O2snHt7f5+e0+cjOTnf8/7++nzybe/rfj/f7/neVBWSpHPbH/S7AZKk/jMMJEmGgSTJMJAkYRhIkoDz+t2Ambrkkktq1apV/W6GJA2URx999OdVteT0+sCGwapVqxgZGel3MyRpoCR5vlfdYSJJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJDHA30DW2WHV1u/NaL0j2z8wxy2RNBueGUiSDANJkmEgScIwkCRhGEiSmEIYJFmZ5KEkTyU5nOTWVr89ybEkh9rr2q51PpVkNMkzSa7uqm9otdEkW7vqlyZ5pNW/meSCue6oJGliUzkzeA34ZFVdBqwDtiS5rM37YlWtaa99AG3eDcA7gA3Al5MsSrII+BJwDXAZcGPXdj7XtvV24CXg5jnqnyRpCs4YBlV1vKoea9O/Ap4Glk+yykZgd1X9pqp+BowCV7TXaFU9V1W/BXYDG5MEeB/wrbb+LuC6mXZIkjR907pmkGQV8C7gkVa6JcnjSXYmWdxqy4EXulY72moT1d8KvFxVr51W77X/zUlGkoyMjY1Np+mSpElMOQySvAm4D/hEVb0C3A28DVgDHAc+Py8t7FJVO6pqbVWtXbLkdX/PWZI0Q1N6HEWS8+kEwdeq6n6AqjrRNf8rwHfbx2PAyq7VV7QaE9R/AVyU5Lx2dtC9vCRpAUzlbqIA9wBPV9UXuurLuhb7IPBkm94L3JDkDUkuBVYDPwYOAqvbnUMX0LnIvLeqCngI+FBbfxPwwOy6JUmajqmcGbwH+DDwRJJDrfZpOncDrQEKOAJ8DKCqDifZAzxF506kLVX1O4AktwD7gUXAzqo63LZ3G7A7yWeBn9AJH0nSAjljGFTVj4D0mLVvknXuBO7sUd/Xa72qeo7O3UaSpD7wG8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRJTCIMkK5M8lOSpJIeT3NrqFyc5kOTZ9r641ZPkriSjSR5PcnnXtja15Z9Nsqmr/u4kT7R17kqS+eisJKm3qZwZvAZ8sqouA9YBW5JcBmwFHqyq1cCD7TPANcDq9toM3A2d8AC2AVcCVwDbxgOkLfPRrvU2zL5rkqSpOmMYVNXxqnqsTf8KeBpYDmwEdrXFdgHXtemNwL3V8TBwUZJlwNXAgao6VVUvAQeADW3eW6rq4aoq4N6ubUmSFsC0rhkkWQW8C3gEWFpVx9usF4GlbXo58ELXakdbbbL60R71XvvfnGQkycjY2Nh0mi5JmsSUwyDJm4D7gE9U1Svd89pv9DXHbXudqtpRVWurau2SJUvme3eSdM6YUhgkOZ9OEHytqu5v5RNtiIf2frLVjwEru1Zf0WqT1Vf0qEuSFshU7iYKcA/wdFV9oWvWXmD8jqBNwANd9ZvaXUXrgF+24aT9wPoki9uF4/XA/jbvlSTr2r5u6tqWJGkBnDeFZd4DfBh4IsmhVvs0sB3Yk+Rm4Hng+jZvH3AtMAr8GvgIQFWdSvIZ4GBb7o6qOtWmPw58FXgj8P32kiQtkDOGQVX9CJjovv/391i+gC0TbGsnsLNHfQR455naIkmaH34DWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJDGFMEiyM8nJJE921W5PcizJofa6tmvep5KMJnkmydVd9Q2tNppka1f90iSPtPo3k1wwlx2UJJ3ZVM4Mvgps6FH/YlWtaa99AEkuA24A3tHW+XKSRUkWAV8CrgEuA25sywJ8rm3r7cBLwM2z6ZAkafrOGAZV9UPg1BS3txHYXVW/qaqfAaPAFe01WlXPVdVvgd3AxiQB3gd8q62/C7humn2QJM3SbK4Z3JLk8TaMtLjVlgMvdC1ztNUmqr8VeLmqXjut3lOSzUlGkoyMjY3NoumSpG4zDYO7gbcBa4DjwOfnrEWTqKodVbW2qtYuWbJkIXYpSeeE82ayUlWdGJ9O8hXgu+3jMWBl16IrWo0J6r8ALkpyXjs76F5ekrRAZnRmkGRZ18cPAuN3Gu0FbkjyhiSXAquBHwMHgdXtzqEL6Fxk3ltVBTwEfKitvwl4YCZtkiTN3BnPDJJ8A7gKuCTJUWAbcFWSNUABR4CPAVTV4SR7gKeA14AtVfW7tp1bgP3AImBnVR1uu7gN2J3ks8BPgHvmrHeSpCk5YxhU1Y09yhP+wK6qO4E7e9T3Aft61J+jc7eRJKlP/AayJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCTiv3w3Q2WHV1u/1uwmS+sgzA0mSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEliCmGQZGeSk0me7KpdnORAkmfb++JWT5K7kowmeTzJ5V3rbGrLP5tkU1f93UmeaOvclSRz3UlJ0uSmcmbwVWDDabWtwINVtRp4sH0GuAZY3V6bgbuhEx7ANuBK4Apg23iAtGU+2rXe6fuSJM2zM4ZBVf0QOHVaeSOwq03vAq7rqt9bHQ8DFyVZBlwNHKiqU1X1EnAA2NDmvaWqHq6qAu7t2pYkaYHM9JrB0qo63qZfBJa26eXAC13LHW21yepHe9QlSQto1heQ22/0NQdtOaMkm5OMJBkZGxtbiF1K0jlhpmFwog3x0N5PtvoxYGXXcitabbL6ih71nqpqR1Wtraq1S5YsmWHTJUmnm2kY7AXG7wjaBDzQVb+p3VW0DvhlG07aD6xPsrhdOF4P7G/zXkmyrt1FdFPXtiRJC+SMf88gyTeAq4BLkhylc1fQdmBPkpuB54Hr2+L7gGuBUeDXwEcAqupUks8AB9tyd1TV+EXpj9O5Y+mNwPfbS5K0gM4YBlV14wSz3t9j2QK2TLCdncDOHvUR4J1naockaf74DWRJkmEgSTIMJElM4ZqBNB9Wbf3ejNY7sv0Dc9wSSeCZgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn4COuhM9NHQ0s6t3lmIEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEj6O4qzlYyUkLSTPDCRJswuDJEeSPJHkUJKRVrs4yYEkz7b3xa2eJHclGU3yeJLLu7azqS3/bJJNs+uSJGm65mKY6K+r6uddn7cCD1bV9iRb2+fbgGuA1e11JXA3cGWSi4FtwFqggEeT7K2ql+agbRoyMxk+O7L9A/PQEmm4zMcw0UZgV5veBVzXVb+3Oh4GLkqyDLgaOFBVp1oAHAA2zEO7JEkTmG0YFPCDJI8m2dxqS6vqeJt+EVjappcDL3Ste7TVJqq/TpLNSUaSjIyNjc2y6ZKkcbMdJnpvVR1L8kfAgST/2T2zqipJzXIf3dvbAewAWLt27ZxtV5LOdbM6M6iqY+39JPBt4ArgRBv+ob2fbIsfA1Z2rb6i1SaqS5IWyIzDIMmFSd48Pg2sB54E9gLjdwRtAh5o03uBm9pdReuAX7bhpP3A+iSL251H61tNkrRAZjNMtBT4dpLx7Xy9qv49yUFgT5KbgeeB69vy+4BrgVHg18BHAKrqVJLPAAfbcndU1alZtEuSNE0zDoOqeg74ix71XwDv71EvYMsE29oJ7JxpWyRJs+PjKDT0ZvpoD7+foHOJj6OQJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRLn6DeQ/UaqJP1/nhlIkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSxDn6Zy8X2kz/zKb6yz+PqnOJZwaSpLPnzCDJBuBfgEXAv1XV9j436XX8DV/SsDorwiDJIuBLwN8AR4GDSfZW1VP9bZk0fQ4vaRCdFWEAXAGMVtVzAEl2AxsBw0DnDENE/XS2hMFy4IWuz0eBK09fKMlmYHP7+GqSZ7pmXwL8fN5a2B/D1qdh6w+cBX3K5+Z0c33vzzwYtj7Ntj9/0qt4toTBlFTVDmBHr3lJRqpq7QI3aV4NW5+GrT8wfH0atv7A8PVpvvpzttxNdAxY2fV5RatJkhbA2RIGB4HVSS5NcgFwA7C3z22SpHPGWTFMVFWvJbkF2E/n1tKdVXV4mpvpOXw04IatT8PWHxi+Pg1bf2D4+jQv/UlVzcd2JUkD5GwZJpIk9ZFhIEkajjBIsiHJM0lGk2ztd3vmQpIjSZ5IcijJSL/bM11JdiY5meTJrtrFSQ4keba9L+5nG6drgj7dnuRYO06HklzbzzZOR5KVSR5K8lSSw0lubfWBPE6T9GeQj9EfJvlxkp+2Pv1Dq1+a5JH2M++b7cab2e1r0K8ZtEdZ/Bddj7IAbhz0R1kkOQKsraqB/LJMkr8CXgXurap3tto/AqeqansL7cVVdVs/2zkdE/TpduDVqvqnfrZtJpIsA5ZV1WNJ3gw8ClwH/B0DeJwm6c/1DO4xCnBhVb2a5HzgR8CtwN8D91fV7iT/Cvy0qu6ezb6G4czgfx9lUVW/BcYfZaE+qqofAqdOK28EdrXpXXT+ow6MCfo0sKrqeFU91qZ/BTxN52kAA3mcJunPwKqOV9vH89urgPcB32r1OTlGwxAGvR5lMdD/AJoCfpDk0fYYjmGwtKqOt+kXgaX9bMwcuiXJ420YaSCGVE6XZBXwLuARhuA4ndYfGOBjlGRRkkPASeAA8N/Ay1X1WltkTn7mDUMYDKv3VtXlwDXAljZEMTSqMz452GOUHXcDbwPWAMeBz/e3OdOX5E3AfcAnquqV7nmDeJx69Gegj1FV/a6q1tB5MsMVwJ/Nx36GIQyG8lEWVXWsvZ8Evk3nH8GgO9HGdcfHd0/2uT2zVlUn2n/W3wNfYcCOUxuHvg/4WlXd38oDe5x69WfQj9G4qnoZeAj4S+CiJONfGp6Tn3nDEAZD9yiLJBe2C2AkuRBYDzw5+VoDYS+wqU1vAh7oY1vmxPgPzeaDDNBxahcn7wGerqovdM0ayOM0UX8G/BgtSXJRm34jnRtlnqYTCh9qi83JMRr4u4kA2q1i/8z/Pcrizj43aVaS/CmdswHoPDLk64PWpyTfAK6i87jdE8A24DvAHuCPgeeB66tqYC7ITtCnq+gMPxRwBPhY13j7WS3Je4H/AJ4Aft/Kn6Yzzj5wx2mS/tzI4B6jP6dzgXgRnV/e91TVHe1nxG7gYuAnwN9W1W9mta9hCANJ0uwMwzCRJGmWDANJkmEgSTIMJEkYBpIkDANJEoaBJAn4H00rN2ghfkoHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZqD05ny9t82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FinetuneDataset(Dataset):\n",
        "  def __init__(self,\n",
        "               data):\n",
        "    self.sos = 3\n",
        "    self.eos = 4\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    x = tokenizer.encode(self.data['text'].values[idx])\n",
        "    target = tokenizer.encode(self.data['title'].values[idx])\n",
        "    attention_mask = [1]*len(x)\n",
        "    \n",
        "    x = torch.tensor(x)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    target = torch.tensor(target)\n",
        "\n",
        "    return {'input':x, 'attn':attention_mask, 'target':target}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBUEU1tYnWpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_fn(records):\n",
        "  max_len_input = min(300, max(len(record['input']) for record in records))\n",
        "  max_len_target = max(len(record['target']) for record in records)\n",
        "  attention_mask = torch.zeros(len(records), max_len_input)\n",
        "  target = torch.zeros(len(records), max_len_target)\n",
        "  input = torch.zeros(len(records), max_len_input)\n",
        "\n",
        "  for i, ten in enumerate(records):\n",
        "    len_ = min(max_len_input, len(ten['input']))\n",
        "    input[i, :len_] += ten['input'][:len_]\n",
        "    input[i][input[i]==0] = 1\n",
        "\n",
        "    target[i, :len(ten['target'])] += ten['target']\n",
        "    target[i][target[i]==0] = 1\n",
        "    \n",
        "    attention_mask[i, :len_] += ten['attn'][:len_]\n",
        "\n",
        "  lm_labels = target[:, 1:].clone()\n",
        "  lm_labels[target[:, 1:] == 1] = -100\n",
        "\n",
        "  target = target[:, :-1].contiguous()\n",
        "\n",
        "  return {'input_ids':input.type(torch.LongTensor),\n",
        "          'attention_mask':attention_mask.type(torch.LongTensor),\n",
        "          'decoder_input_ids':target.type(torch.LongTensor),\n",
        "          'lm_labels':lm_labels.type(torch.LongTensor)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCyI1fOW04rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = FinetuneDataset(X_train_rio)\n",
        "val_dataset = FinetuneDataset(X_val_rio)\n",
        "test_dataset = FinetuneDataset(X_test_rio)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX3HJ7wj09yA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle=False, collate_fn=collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVEBpPHxp_Np",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_calc(logits, labels, smoothing = True):\n",
        "  labels = labels[:, 1:]\n",
        "  logits = logits.reshape(-1, logits.size(-1))\n",
        "  labels = labels.reshape(-1)\n",
        "\n",
        "  if smoothing:\n",
        "    eps = 0.1\n",
        "    num_classes = logits.size(-1)\n",
        "\n",
        "    log_preds = F.log_softmax(logits, dim=-1)\n",
        "    loss = -log_preds.sum(dim=-1).mean()\n",
        "    nll = F.nll_loss(log_preds, labels)\n",
        "    loss = eps*loss/num_classes + (1-eps)*nll\n",
        "\n",
        "  else:\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6sDv9ek5Qvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class BART_finetune(LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(BART_finetune, self).__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.bart = model_pretrained\n",
        "\n",
        "    def forward(self, decoder_input_ids = None, input_ids=None, attention_mask=None, lm_labels = None):\n",
        "        bart_output = self.bart(input_ids=input_ids,\n",
        "                                attention_mask=attention_mask,\n",
        "                                decoder_input_ids = decoder_input_ids,\n",
        "                                lm_labels = lm_labels)\n",
        "        return bart_output[0]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(train_dataset, batch_size=self.hparams.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(val_dataset, batch_size=self.hparams.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(test_dataset, batch_size=self.hparams.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = transformers.AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.7)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self(**batch)\n",
        "\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        \n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self(**batch)\n",
        "\n",
        "        #output_dim = logits.shape[-1]\n",
        "        #predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        #ref = tokenizer.batch_decode(y)\n",
        "        #pred = tokenizer.batch_decode(predictions)\n",
        "        #bleu = corpus_bleu([[r] for r in ref], pred) \n",
        "\n",
        "        #acc = accuracy_score(torch.argmax(logits.reshape(-1, output_dim), dim=-1).cpu(), y[:, 1:].reshape(-1).cpu())\n",
        "        #acc = torch.tensor(acc)\n",
        "        #bleu = torch.FloatTensor([bleu])\n",
        "\n",
        "        return {'val_loss': loss}\n",
        "    \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        #avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "        #avg_bleu = torch.stack([x['bleu'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs}\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self(**batch)\n",
        "\n",
        "        #output_dim = logits.shape[-1]\n",
        "        #predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        #ref = tokenizer.batch_decode(y)\n",
        "        #pred = tokenizer.batch_decode(predictions)\n",
        "        #ref_b = [' '.join([tokenizer.decode(i)]) for i in y]\n",
        "        #pred_b = [' '.join([tokenizer.decode(i)]) for i in predictions]\n",
        "\n",
        "        #bleu = corpus_bleu([[r] for r in ref], pred) \n",
        "\n",
        "        #rouge = Rouge()\n",
        "        #rouge = rouge.get_scores(pred_b, ref_b, avg=True, ignore_empty = True)\n",
        "        #rouge_2_f, rouge_l_f = torch.tensor(rouge['rouge-2']['f']), torch.tensor(rouge['rouge-l']['f'])\n",
        "\n",
        "        #acc = accuracy_score(torch.argmax(logits.reshape(-1, output_dim), dim=-1).cpu(), y[:, 1:].reshape(-1).cpu())\n",
        "        #acc = torch.tensor(acc)\n",
        "        #bleu = torch.FloatTensor([bleu])\n",
        "\n",
        "        return {'test_loss': loss}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "        #avg_val_test = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
        "        #avg_bleu = torch.stack([x['bleu'] for x in outputs]).mean()\n",
        "        \n",
        "        #rouge_2 = torch.stack([x['rouge-2'] for x in outputs]).mean()\n",
        "        #rouge_l = torch.stack([x['rouge-l'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'test_loss': avg_loss}\n",
        "        return {'test_loss': avg_loss, 'progress_bar': tensorboard_logs}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrJss-245Q1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning import seed_everything\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from argparse import Namespace\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='/content/drive/My Drive/Huawei/lightning_logs/finetune with pretrain gazeta',\n",
        "    save_top_k=True,\n",
        "    verbose=True,\n",
        "    monitor='avg_bleu',\n",
        "    mode='max',\n",
        "    prefix=''\n",
        ")\n",
        "\n",
        "seed_everything(42)\n",
        "args = {\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'lr': 0.0004,\n",
        "    'num_epoch':10,\n",
        "}\n",
        "hparams = Namespace(**args)\n",
        "\n",
        "model = BART_finetune(hparams)\n",
        "trainer = Trainer(gpus=1, max_epochs=15,deterministic=True, auto_lr_find=True)\n",
        "trainer.fit(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCp3QrgOHqFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhNFx7afUQWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmHIuIJO5Q6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref = []\n",
        "pred = []\n",
        "for i, args in enumerate(test_loader):\n",
        "  pred.append(tokenizer.decode(model.bart.generate(args['input_ids']).squeeze()))\n",
        "  ref.append(tokenizer.decode(args['decoder_input_ids'].squeeze()))\n",
        "calc_scores(ref, pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usw7DvpOmjMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, args in enumerate(test_loader):\n",
        "  print(80*'-')\n",
        "  print('text:', tokenizer.batch_decode(args['input_ids'])[0])\n",
        "  print('predicted:', tokenizer.decode(model.bart.generate(args['input_ids']).squeeze()))\n",
        "  print('reference:', tokenizer.decode(args['decoder_input_ids'].squeeze()))\n",
        "  if i==10:break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga9xbfVjWadg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqJKRxlwWaaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KlfLuj9J5JV",
        "colab_type": "text"
      },
      "source": [
        "## Extractive method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2lsdtc8L-XM",
        "colab_type": "text"
      },
      "source": [
        "I will try to use extractive method so as to compare with abstractive method.\n",
        "Extractive methods, obviously, extract sentences from the text, so we need labeled data, but it is tough. In order to get these labels I will use oracle algorithm, which select sentences that are best suited to reference summary.\n",
        "For this task I will use SummerRunner with pretrained sentence embeddings.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i46lrY-VJ-9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install razdel\n",
        "!pip install catalyst\n",
        "!pip install rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vu-EBD6KU17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import random\n",
        "import razdel\n",
        "\n",
        "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
        "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
        "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
        "    n_sentences = len(sentences)\n",
        "    oracle_summary_sentences = set()\n",
        "    score = -1.0\n",
        "    summaries = []\n",
        "    for _ in range(min(n_sentences, 2)):\n",
        "        for i in range(n_sentences):\n",
        "            if i in oracle_summary_sentences:\n",
        "                continue\n",
        "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
        "            # Add sentence to summary\n",
        "            current_summary_sentences.add(i)\n",
        "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
        "            # Calculate metrics\n",
        "            current_score = calc_score(current_summary, gold_summary)\n",
        "            summaries.append((current_score, current_summary_sentences))\n",
        "        # If the mectrics have been updated, add more\n",
        "        # Else finish\n",
        "        best_summary_score, best_summary_sentences = max(summaries)\n",
        "        if best_summary_score <= score:\n",
        "            break\n",
        "        oracle_summary_sentences = best_summary_sentences\n",
        "        score = best_summary_score\n",
        "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
        "    return oracle_summary, oracle_summary_sentences\n",
        "\n",
        "def calc_single_score(pred_summary, gold_summary, rouge):\n",
        "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57fFyGJ-NmrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "word_embedding_model = models.Transformer('DeepPavlov/rubert-base-cased-sentence')\n",
        "\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSxaqAU8Nmw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rouge import Rouge\n",
        "import razdel\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
        "    rouge = Rouge()\n",
        "    sentences_ = []\n",
        "    oracle_sentences_ = []\n",
        "    oracle_summary_ = []\n",
        "    records = records.iloc[:nrows].copy()\n",
        "\n",
        "    for text, summary in tqdm(records[['text', 'summary']].values):\n",
        "        summary = summary.lower() if lower else summary\n",
        "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
        "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
        "                                                                         lower=lower, max_sentences=max_sentences)\n",
        "        sentences_ += [sentences]\n",
        "        oracle_sentences_ += [list(sentences_indicies)]\n",
        "        oracle_summary_ += [oracle_summary]\n",
        "    records['sentences'] = sentences_\n",
        "    records['oracle_sentences'] = oracle_sentences_\n",
        "    records['oracle_summary'] = oracle_summary_\n",
        "    return records\n",
        "\n",
        "ext_train_records = add_oracle_summary_to_records(train_gazeta, nrows=10000)\n",
        "ext_val_records = add_oracle_summary_to_records(val_gazeta, nrows=1000)\n",
        "ext_test_records = add_oracle_summary_to_records(test_gazeta, nrows=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFKVBa2wNm0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import math\n",
        "import razdel\n",
        "import torch\n",
        "import numpy as np\n",
        "from rouge import Rouge\n",
        "\n",
        "\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "class ExtDataset(data.Dataset):\n",
        "    def __init__(self, records, vocabulary, lower=True,max_sentence_length=50, device=torch.device('cuda')):\n",
        "        self.records = records\n",
        "        self.num_samples = records.shape[0]\n",
        "        self.lower = lower\n",
        "        self.rouge = Rouge()\n",
        "        self.vocabulary = vocabulary\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        self.device = device\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.records.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_record = self.records.iloc[idx]\n",
        "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
        "\n",
        "        return {'inputs': cur_record['sentences'], 'outputs': outputs}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iu6N6eqNm3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_fn(records):\n",
        "    max_sentences = max(len(record['outputs']) for record in records)\n",
        "\n",
        "    inputs = torch.zeros(len(records), max_sentences, 768)\n",
        "    outputs = torch.zeros((len(records), max_sentences))\n",
        "    for i, record in enumerate(records):\n",
        "      inputs[i, :len(record['inputs'])] += torch.tensor(encoder.encode(record['inputs']))\n",
        "      outputs[i, :len(record['outputs'])] += np.array(record['outputs'])\n",
        "            \n",
        "    return {'inputs': inputs, 'targets': outputs.type(torch.FloatTensor)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPrvR9drPK41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
        "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
        "\n",
        "\n",
        "class SentenceEncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n",
        "        super(SentenceEncoderRNN, self).__init__()\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n",
        "        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        embedded = self.embedding_layer(inputs)\n",
        "        outputs, _ = self.rnn_layer(embedded, hidden)\n",
        "        sentences_embeddings = torch.mean(outputs, 1)\n",
        "        return sentences_embeddings\n",
        "\n",
        "\n",
        "class SentenceTaggerRNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 token_embedding_dim=256,\n",
        "                 sentence_encoder_hidden_size=768,\n",
        "                 hidden_size=256,\n",
        "                 bidirectional=True,\n",
        "                 sentence_encoder_n_layers=2,\n",
        "                 sentence_encoder_dropout=0.3,\n",
        "                 sentence_encoder_bidirectional=True,\n",
        "                 n_layers=1,\n",
        "                 dropout=0.3):\n",
        "        super(SentenceTaggerRNN, self).__init__()\n",
        "\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        assert hidden_size % num_directions == 0\n",
        "        hidden_size = hidden_size // num_directions\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n",
        "                           bidirectional=bidirectional, batch_first=True)\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
        "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
        "        self.tanh_layer = nn.Tanh()\n",
        "\n",
        "    def forward(self, inputs, hidden = None):\n",
        "        batch_size = inputs.size(0)\n",
        "        sentences_count = inputs.size(1)\n",
        "        outputs, _ = self.rnn_layer(inputs, hidden)\n",
        "        outputs = self.dropout_layer(outputs)\n",
        "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
        "        content = self.content_linear_layer(outputs).squeeze(2)\n",
        "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2)\n",
        "        return content + salience\n",
        "\n",
        "model = SentenceTaggerRNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjB6mvJXPLfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import catalyst\n",
        "from catalyst.dl.runner import SupervisedRunner\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "loaders = {\n",
        "    'train': data.DataLoader(ExtDataset(ext_train_records, None), batch_size=32, collate_fn=collate_fn),\n",
        "    'valid': data.DataLoader(ExtDataset(ext_val_records, None), batch_size=32, collate_fn=collate_fn),\n",
        "    'test': data.DataLoader(ExtDataset(ext_test_records, None), batch_size=32, collate_fn=collate_fn),\n",
        "}\n",
        "\n",
        "lr = 1e-3\n",
        "num_epochs = 2\n",
        "\n",
        "optimizer  = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "runner = SupervisedRunner(\n",
        "    input_key=(\n",
        "        \"inputs\"\n",
        "    )\n",
        ")\n",
        "runner.train(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    logdir='./logs',\n",
        "    num_epochs=num_epochs,\n",
        "    criterion=criterion,\n",
        "    verbose=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxv63kLdPLlX",
        "colab_type": "code",
        "outputId": "19dfcc1d-a70f-4cf4-8807-5562d0723476",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "references = []\n",
        "predictions = []\n",
        "model.eval()\n",
        "for i, item in tqdm(enumerate(data.DataLoader(ExtDataset(ext_test_records, None), batch_size=1, collate_fn=collate_fn)), total=ext_test_records.shape[0]):\n",
        "    logits = model(item[\"inputs\"].to(device))[0] # Прямой проход\n",
        "    record = ext_test_records.iloc[i]\n",
        "    predicted_summary = []\n",
        "    for i, logit in enumerate(logits):\n",
        "        if logit > 0.15:\n",
        "            predicted_summary.append(record['sentences'][i])\n",
        "    if not predicted_summary:\n",
        "        predicted_summary.append(record['sentences'][torch.max(logits, dim=0)[1].item()])\n",
        "    predicted_summary = \" \".join(predicted_summary)\n",
        "    references.append(record['summary'].lower())\n",
        "    predictions.append(predicted_summary)\n",
        "\n",
        "calc_scores(references, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:03<00:00, 15.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Ref: в азербайджане 9 февраля прошли внеочередные парламентские выборы. по итогам обработки 87% протоколов победу одерживает правящая партия «ени азербайджан», она может получить до 65 мест из 125. внеочередные выборы прошли в рамках новой кадровой политики президента, призванной ускорить проведение реформ. глава государства в конце прошлого года почти полностью обновил кабмин. серьезные правительственные кадровые изменения становятся тенденцией на постсоветском пространстве.\n",
            "Hyp: 9 февраля в азербайджане прошли первые в истории страны внеочередные парламентские выборы. правящая партия «ени азербайджан» уже объявила о своей победе.\n",
            "BLEU:  0.4145022687253822\n",
            "ROUGE:  {'rouge-1': {'f': 0.24765908811075263, 'p': 0.30595039390157663, 'r': 0.24025562750931886}, 'rouge-2': {'f': 0.10928223237916704, 'p': 0.13632797476823622, 'r': 0.10623049944846108}, 'rouge-l': {'f': 0.2228707656753949, 'p': 0.27253872596430306, 'r': 0.21433428110953515}}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Metrics"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLwJnQLjRI9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
